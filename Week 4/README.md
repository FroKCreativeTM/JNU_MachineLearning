
# 4주차 실습과제

## 이번 주 학습 및 실습 주제
다중 클래스 분류
나이브 베이지안
결정트리
랜덤 포레스트

### 다중 클래스 분류
일대 다 방식으로써, 그룹을 나눈다. 클래스의 수만큼 이진 분류기 사용하고,  클래스마다 가중치와 절편 생성한 뒤 이 중에서 가장 높은 점수의 클래스 선택한다.</br>
또한 오차행렬을 두어서, 좀 더 세분화된 참과 거짓을 판별한다.

### 나이브 베이지안
패턴인식 및 기계학습에선 통계학적인 여러 가지 기법들을 이용한다. 이 때 각 특성값이 서로 독립 가정.

##### 베이즈의 정리 (Bayes’ Theorem)
B1, B2, ..., BN과 같이 상호배타적인 합집합이 표본 공간 S인 경우, A 사건이 일어났을 때 Bj 사건이 일어날 확률을 구할 확률

### 결정트리
이번 실습에서 제일 직관적이고, 시각화가 쉬워서, 제일 이해가 쉬웠던 파트이다. </br>
각 훈련 데이터에 대한 경계값을 트리로 나타내는 방식으로, 트리의 level이 깊어질수록, 훈련 데이터는 굉장히 우수한 확률을 보이지만 반대로, 오히려 테스트 값은 떨어지는 경우 또한 확인할 수 있었다.
이러한 점을 과대적합이라고 하는데, 이 과대적합을 막기위해. MAX_DEPTH를 어떻게 설정하는가는 프로그래머의 몫이다.</br>
또한 각 특성이 얼마나 중요한지에 대해 0~1 사이의 값으로 나타내는 방식이 있는데, 이를 특성 중요도라고 한다.

### 랜덤 포레스트
#### Ensemble
학습의 결과는 noise, variance, bias 등의 요건에 의해 달라지는데, 앙상블은 이중 variance, bias 등의 영향을 줄여주는 역할을 한다.
##### Bagging
여러 개의 bootstrap 자료를 생성하고 결합하여 최종예측모형을 산출한다. 예측모형의 변동성이 큰 경우 variance 감소 목적으로 하며, 과대적합된 모형, bias작고, 분산이 큰 모형에 적합하다.
random forest model이라고도 한다.
##### Boosting
잘못 분류된 개체들에 집중하여 새로운 분류 규칙 수립하는 방식으로, 약한 모델을 결합하여 강한 예측 모형 수립하여, 전 단계의 모델에서 실수를 줄여나간다.
이를 통해서 반복을 줄여, 시간이 줄어든다.

##### 다시 랜덤 포레스트
이 랜덤 포레스트는 이 위에서 설명한 요소들을 활용하여, 과대적합을 피해 결정 트리의 단점을 보완하는 방식이다.
특히 특성의 중요도를 파악하기에, 랜덤 포레스트가 결정트리보다 더 좋은데, 그 이유는 바로, 그래프 상에서 어떤 점이 중요한 지가, 훨씬 더 도드라지게 보이기 때문이다.

하지만 무조건적인 장점만 있는 것은 아니다. 결정 트리보다 더 많은 트리가 생성되어, 자세한 분석이 힘들다는 점, 그리고 트리 시각화는 힘들다는 점이 그 예이다.

##### 그레디언트 부스팅 회귀결정 트리
다수의 결정 트리 기반 앙상블을 이용하는 모델로, 5 이하 깊이의 트리(약한 학습기)로 메모리 사용이 적고 예측이 빠르다라는 장점이 있는 랜덤 포레스트의 단점을 보안한 모델이다.
